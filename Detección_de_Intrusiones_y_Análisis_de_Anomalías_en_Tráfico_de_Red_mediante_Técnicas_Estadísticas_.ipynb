{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4292833",
   "metadata": {},
   "source": [
    "# Detecci√≥n de Intrusiones y An√°lisis de Anomal√≠as en Tr√°fico de Red mediante T√©cnicas Estad√≠sticas\n",
    "\n",
    "**Universidad de La Habana, MATCOM**  \n",
    "**Curso:** Estad√≠stica 2025-2026  \n",
    "**Proyecto Final:** An√°lisis Estad√≠stico Aplicado a Seguridad Inform√°tica\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afbcff",
   "metadata": {},
   "source": [
    "## 1. Introducci√≥n al Proyecto\n",
    "\n",
    "### 1.1 Contexto y Motivaci√≥n\n",
    "\n",
    "La seguridad inform√°tica es uno de los pilares fundamentales en la infraestructura tecnol√≥gica moderna. Los Sistemas de Detecci√≥n de Intrusiones (IDS) tradicionales, basados en firmas conocidas, presentan limitaciones significativas frente a ataques emergentes o modificados (zero-day attacks). \n",
    "\n",
    "Este proyecto propone un enfoque complementario basado en **an√°lisis estad√≠stico del comportamiento del tr√°fico de red**, permitiendo identificar patrones an√≥malos sin depender exclusivamente de firmas previamente catalogadas. Este tipo de aproximaci√≥n resulta especialmente relevante en entornos din√°micos donde los ataques evolucionan constantemente.\n",
    "\n",
    "### 1.2 Objetivos del An√°lisis\n",
    "\n",
    "Este estudio busca responder **tres preguntas de investigaci√≥n fundamentales**:\n",
    "\n",
    "**Pregunta 1 (An√°lisis Comparativo):** ¬øExisten diferencias estad√≠sticamente significativas en el comportamiento de variables de flujo de red ‚Äîcomo `src_bytes`, `dst_bytes` y `duration`‚Äî entre el tr√°fico normal y los distintos tipos de ataques (DoS, Probe, R2L y U2R)?\n",
    "\n",
    "**Pregunta 2 (Reducci√≥n Dimensional):** ¬øEs posible reducir la dimensionalidad de las 41 caracter√≠sticas del tr√°fico de red mediante An√°lisis de Componentes Principales (PCA), conservando al menos el 95% de la varianza explicada, y c√≥mo impacta esta reducci√≥n en la visualizaci√≥n y separaci√≥n de los distintos tipos de ataques?\n",
    "\n",
    "**Pregunta 3 (Clasificaci√≥n Comparativa):** ¬øQu√© t√©cnica de clasificaci√≥n estad√≠stica, Regresi√≥n Log√≠stica o K-Vecinos m√°s Cercanos (K-NN), ofrece una mayor sensibilidad para detectar ataques raros (como U2R) en comparaci√≥n con ataques volum√©tricos m√°s comunes (como DoS)?\n",
    "\n",
    "### 1.3 Dataset: NSL-KDD\n",
    "\n",
    "**Fuente:** [NSL-KDD en Kaggle](https://www.kaggle.com/datasets/hassan06/nslkdd)\n",
    "\n",
    "El dataset NSL-KDD es una versi√≥n refinada del cl√°sico KDD Cup 1999, dise√±ada espec√≠ficamente para eliminar redundancias y sesgos presentes en el conjunto original. Es ampliamente reconocido como est√°ndar acad√©mico para la evaluaci√≥n de algoritmos de detecci√≥n de intrusiones.\n",
    "\n",
    "**Caracter√≠sticas principales:**\n",
    "- **41 variables predictoras** + 1 variable objetivo (`attack_type`) + 1 nivel de dificultad\n",
    "- **Tipos de ataques:** Normal, DoS (Denial of Service), Probe (escaneo/sondeo), R2L (Remote to Local), U2R (User to Root)\n",
    "- **Conjunto de entrenamiento:** 25,192 observaciones\n",
    "- **Conjunto de prueba:** 22,544 observaciones\n",
    "\n",
    "**Categorizaci√≥n de variables:**\n",
    "- **B√°sicas:** Derivadas de cabeceras TCP/IP (duration, protocol_type, src_bytes, dst_bytes, flag)\n",
    "- **De contenido:** Informaci√≥n sobre el payload (num_failed_logins, root_shell, etc.)\n",
    "- **De tr√°fico:** Estad√≠sticas temporales orientadas a detectar patrones (count, serror_rate, etc.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60a8ba5",
   "metadata": {},
   "source": [
    "## 1.4 Carga de Datos y Preparaci√≥n Inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab919d",
   "metadata": {},
   "source": [
    "### Imports y Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432cd5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaci√≥n de librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Configuraci√≥n de visualizaciones\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Estilo global para mantener consistencia en todas las visualizaciones\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 13\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# Paleta de colores consistente para categor√≠as de ataque\n",
    "# Se utilizar√° en todas las visualizaciones del proyecto\n",
    "attack_colors = {\n",
    "    'Normal': '#2ecc71',    # Verde - Tr√°fico leg√≠timo\n",
    "    'DoS': '#e74c3c',       # Rojo - Ataques de denegaci√≥n de servicio\n",
    "    'Probe': '#f39c12',     # Naranja - Ataques de reconocimiento\n",
    "    'R2L': '#9b59b6',       # Morado - Acceso remoto no autorizado\n",
    "    'U2R': '#34495e'        # Gris oscuro - Escalada de privilegios\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas y configuraci√≥n de visualizaci√≥n establecida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534817e",
   "metadata": {},
   "source": [
    "### Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc41226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los nombres de las columnas (43 columnas en total)\n",
    "col_names = [\n",
    "    \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
    "    \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\",\n",
    "    \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\",\n",
    "    \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\",\n",
    "    \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\",\n",
    "    \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n",
    "    \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
    "    \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\",\n",
    "    \"dst_host_srv_count\", \"dst_host_same_srv_rate\",\n",
    "    \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\",\n",
    "    \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\",\n",
    "    \"dst_host_srv_rerror_rate\", \"attack_type\", \"difficulty_level\"\n",
    "]\n",
    "\n",
    "# Cargar los datasets\n",
    "# Nota: Ajusta las rutas seg√∫n tu estructura de carpetas\n",
    "train_df = pd.read_csv('Data/KDDTrain+_20Percent.txt', \n",
    "                       names=col_names, \n",
    "                       header=None)\n",
    "\n",
    "test_df = pd.read_csv('Data/KDDTest+.txt', \n",
    "                      names=col_names, \n",
    "                      header=None)\n",
    "\n",
    "# Crear variable binaria para clasificaci√≥n binaria (Normal vs. Ataque)\n",
    "train_df['is_attack'] = (train_df['attack_type'] != 'normal').astype(int)\n",
    "test_df['is_attack'] = (test_df['attack_type'] != 'normal').astype(int)\n",
    "\n",
    "# Mostrar informaci√≥n b√°sica\n",
    "print(f\"üìä Datos de entrenamiento: {train_df.shape}\")\n",
    "print(f\"üìä Datos de prueba: {test_df.shape}\")\n",
    "print(f\"\\n‚úÖ Datasets cargados exitosamente\")\n",
    "\n",
    "# Distribuci√≥n binaria inicial\n",
    "print(f\"\\nüéØ Distribuci√≥n binaria en entrenamiento (Normal vs. Ataque):\")\n",
    "print(train_df['is_attack'].value_counts(normalize=True).round(4))\n",
    "\n",
    "print(f\"\\nüéØ Distribuci√≥n binaria en prueba (Normal vs. Ataque):\")\n",
    "print(test_df['is_attack'].value_counts(normalize=True).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f34c121",
   "metadata": {},
   "source": [
    "## 1.5 Mapeo de Categor√≠as de Ataque\n",
    "\n",
    "El dataset NSL-KDD contiene 39 tipos de ataques espec√≠ficos que se agrupan en 4 categor√≠as principales m√°s la clase normal. A continuaci√≥n se realiza el mapeo oficial seg√∫n la documentaci√≥n del Canadian Institute for Cybersecurity:\n",
    "\n",
    "- **Normal:** Tr√°fico de red leg√≠timo\n",
    "- **DoS (Denial of Service):** Ataques que buscan denegar el servicio mediante sobrecarga de recursos\n",
    "- **Probe (Probing/Scanning):** Ataques de reconocimiento que escanean la red en busca de vulnerabilidades\n",
    "- **R2L (Remote to Local):** Intentos de acceso no autorizado desde una m√°quina remota\n",
    "- **U2R (User to Root):** Intentos de escalada de privilegios de usuario normal a superusuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1818a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario oficial de mapeo de ataques espec√≠ficos a categor√≠as generales\n",
    "# Fuente: Documentaci√≥n oficial NSL-KDD (Canadian Institute for Cybersecurity)\n",
    "attack_category_mapping = {\n",
    "    # Tr√°fico Normal\n",
    "    'normal': 'Normal',\n",
    "    \n",
    "    # DoS (Denial of Service) - Ataques de denegaci√≥n de servicio\n",
    "    'back': 'DoS', 'land': 'DoS', 'neptune': 'DoS', 'pod': 'DoS',\n",
    "    'smurf': 'DoS', 'teardrop': 'DoS', 'mailbomb': 'DoS', 'apache2': 'DoS',\n",
    "    'processtable': 'DoS', 'udpstorm': 'DoS',\n",
    "    \n",
    "    # Probe (Probing/Scanning) - Ataques de reconocimiento\n",
    "    'ipsweep': 'Probe', 'nmap': 'Probe', 'portsweep': 'Probe',\n",
    "    'satan': 'Probe', 'mscan': 'Probe', 'saint': 'Probe',\n",
    "    \n",
    "    # R2L (Remote to Local) - Acceso no autorizado desde m√°quina remota\n",
    "    'ftp_write': 'R2L', 'guess_passwd': 'R2L', 'imap': 'R2L',\n",
    "    'multihop': 'R2L', 'phf': 'R2L', 'spy': 'R2L',\n",
    "    'warezclient': 'R2L', 'warezmaster': 'R2L', 'sendmail': 'R2L',\n",
    "    'named': 'R2L', 'snmpgetattack': 'R2L', 'snmpguess': 'R2L',\n",
    "    'xlock': 'R2L', 'xsnoop': 'R2L', 'worm': 'R2L',\n",
    "    \n",
    "    # U2R (User to Root) - Escalada de privilegios\n",
    "    'buffer_overflow': 'U2R', 'loadmodule': 'U2R', 'perl': 'U2R',\n",
    "    'rootkit': 'U2R', 'httptunnel': 'U2R', 'ps': 'U2R', 'sqlattack': 'U2R'\n",
    "}\n",
    "\n",
    "# Aplicar mapeo a ambos datasets\n",
    "train_df['attack_category'] = train_df['attack_type'].map(attack_category_mapping)\n",
    "test_df['attack_category'] = test_df['attack_type'].map(attack_category_mapping)\n",
    "\n",
    "# Verificar que no hay valores sin mapear\n",
    "print(\"üîç Verificaci√≥n de mapeo de categor√≠as:\")\n",
    "unmapped_train = train_df['attack_category'].isna().sum()\n",
    "unmapped_test = test_df['attack_category'].isna().sum()\n",
    "print(f\"   Valores sin mapear en train: {unmapped_train}\")\n",
    "print(f\"   Valores sin mapear en test: {unmapped_test}\")\n",
    "\n",
    "if unmapped_test > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è ADVERTENCIA: Hay {unmapped_test} ataques en test sin categor√≠a asignada.\")\n",
    "    print(\"   Esto es esperado en NSL-KDD, que incluye ataques nuevos en el conjunto de prueba.\")\n",
    "    print(\"   Estos registros se filtrar√°n en an√°lisis posteriores.\")\n",
    "\n",
    "# Crear orden categ√≥rico para visualizaciones consistentes\n",
    "category_order = ['Normal', 'DoS', 'Probe', 'R2L', 'U2R']\n",
    "train_df['attack_category'] = pd.Categorical(\n",
    "    train_df['attack_category'], \n",
    "    categories=category_order, \n",
    "    ordered=True\n",
    ")\n",
    "test_df['attack_category'] = pd.Categorical(\n",
    "    test_df['attack_category'], \n",
    "    categories=category_order, \n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Mostrar distribuci√≥n de categor√≠as\n",
    "print(\"\\nüìä Distribuci√≥n de categor√≠as en ENTRENAMIENTO:\")\n",
    "category_dist_train = train_df['attack_category'].value_counts()\n",
    "category_pct_train = train_df['attack_category'].value_counts(normalize=True) * 100\n",
    "category_summary_train = pd.DataFrame({\n",
    "    'Frecuencia': category_dist_train,\n",
    "    'Porcentaje': category_pct_train.round(2)\n",
    "})\n",
    "print(category_summary_train)\n",
    "\n",
    "print(\"\\nüìä Distribuci√≥n de categor√≠as en PRUEBA:\")\n",
    "category_dist_test = test_df[test_df['attack_category'].notna()]['attack_category'].value_counts()\n",
    "category_pct_test = test_df[test_df['attack_category'].notna()]['attack_category'].value_counts(normalize=True) * 100\n",
    "category_summary_test = pd.DataFrame({\n",
    "    'Frecuencia': category_dist_test,\n",
    "    'Porcentaje': category_pct_test.round(2)\n",
    "})\n",
    "print(category_summary_test)\n",
    "\n",
    "print(\"\\n‚úÖ Mapeo de categor√≠as completado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78856dd",
   "metadata": {},
   "source": [
    "## 1.6 Preparaci√≥n de Muestra Estratificada para Visualizaciones\n",
    "\n",
    "Para optimizar el rendimiento de visualizaciones complejas (como scatterplot matrices y pairplots), crearemos una muestra estratificada de 5,000 observaciones que mantenga las proporciones originales de cada categor√≠a de ataque. \n",
    "\n",
    "**Nota importante:** Esta muestra se utilizar√° **exclusivamente para visualizaciones**. Todos los an√°lisis estad√≠sticos (correlaciones, pruebas de hip√≥tesis, modelos) se realizar√°n sobre el dataset completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77858328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n del muestreo estratificado\n",
    "sample_size = 5000\n",
    "random_state = 42  # Para reproducibilidad\n",
    "\n",
    "# Crear muestra estratificada manteniendo proporciones de cada categor√≠a\n",
    "train_sample = train_df.groupby('attack_category', group_keys=False).apply(\n",
    "    lambda x: x.sample(\n",
    "        n=min(len(x), int(sample_size * len(x) / len(train_df))),\n",
    "        random_state=random_state\n",
    "    )\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"üìä Muestra estratificada creada: {len(train_sample):,} observaciones\")\n",
    "print(f\"\\n‚úÖ Verificaci√≥n de estratificaci√≥n:\")\n",
    "\n",
    "# Comparar proporciones originales vs. muestra\n",
    "comparison = pd.DataFrame({\n",
    "    'Original (%)': train_df['attack_category'].value_counts(normalize=True).sort_index() * 100,\n",
    "    'Muestra (%)': train_sample['attack_category'].value_counts(normalize=True).sort_index() * 100\n",
    "})\n",
    "comparison['Diferencia (pp)'] = (comparison['Muestra (%)'] - comparison['Original (%)']).abs()\n",
    "print(comparison.round(2))\n",
    "\n",
    "print(\"\\nüí° Esta muestra se usar√° √∫nicamente para visualizaciones pesadas.\")\n",
    "print(\"   Los an√°lisis estad√≠sticos utilizar√°n el dataset completo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91279fc6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. AN√ÅLISIS EXPLORATORIO DE DATOS (EDA)\n",
    "\n",
    "El An√°lisis Exploratorio de Datos es fundamental para comprender la estructura, distribuci√≥n y relaciones presentes en el dataset antes de aplicar t√©cnicas estad√≠sticas avanzadas. Esta secci√≥n cumple tres objetivos principales:\n",
    "\n",
    "1. **Validar la calidad de los datos** y detectar problemas estructurales\n",
    "2. **Fundamentar decisiones de preparaci√≥n** que se implementar√°n en la secci√≥n 3\n",
    "3. **Generar hip√≥tesis preliminares** que guiar√°n las pruebas estad√≠sticas posteriores\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60456896",
   "metadata": {},
   "source": [
    "## 2.1 Informaci√≥n General del Dataset\n",
    "\n",
    "En esta primera secci√≥n del EDA realizaremos una exploraci√≥n estructural del dataset para verificar:\n",
    "- Dimensiones y tipos de datos\n",
    "- Presencia de valores nulos o faltantes\n",
    "- Distribuci√≥n de variables num√©ricas vs. categ√≥ricas\n",
    "- Uso de memoria, consideraciones computacionales y duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INFORMACI√ìN ESTRUCTURAL DEL DATASET\")\n",
    "\n",
    "# 1. Dimensiones\n",
    "print(f\"\\nüì¶ Dimensiones de los datasets:\")\n",
    "print(f\"   Entrenamiento: {train_df.shape[0]:,} observaciones √ó {train_df.shape[1]} variables\")\n",
    "print(f\"   Prueba: {test_df.shape[0]:,} observaciones √ó {test_df.shape[1]} variables\")\n",
    "\n",
    "# 2. Informaci√≥n de tipos de datos\n",
    "print(f\"\\nüî¢ Distribuci√≥n de tipos de datos (Train):\")\n",
    "print(train_df.dtypes.value_counts())\n",
    "\n",
    "# 3. Duplicados\n",
    "print(f\"\\nüîÅ REGISTROS DUPLICADOS:\")\n",
    "duplicates = train_df.duplicated().sum()\n",
    "print(f\"Filas duplicadas: {duplicates}\")\n",
    "\n",
    "# 4. Separaci√≥n de variables num√©ricas y categ√≥ricas\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Remover variables objetivo y auxiliares de las listas\n",
    "numeric_cols = [col for col in numeric_cols if col not in ['is_attack', 'difficulty_level']]\n",
    "categorical_cols = [col for col in categorical_cols if col not in ['attack_type', 'attack_category']]\n",
    "\n",
    "print(f\"\\nüìä Clasificaci√≥n de variables predictoras:\")\n",
    "print(f\"   Variables num√©ricas: {len(numeric_cols)}\")\n",
    "print(f\"   Variables categ√≥ricas: {len(categorical_cols)}\")\n",
    "print(f\"   Total de predictoras: {len(numeric_cols) + len(categorical_cols)}\")\n",
    "\n",
    "print(f\"\\n   Variables categ√≥ricas identificadas:\")\n",
    "for col in categorical_cols:\n",
    "    n_unique = train_df[col].nunique()\n",
    "    print(f\"      - {col}: {n_unique} categor√≠as √∫nicas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c378732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificaci√≥n de valores nulos en variables predictoras originales\n",
    "print(\"\\nüîç Verificaci√≥n de valores faltantes:\")\n",
    "\n",
    "# Excluir attack_category del an√°lisis de nulos (es variable derivada)\n",
    "original_cols = [col for col in train_df.columns if col not in ['attack_category', 'is_attack']]\n",
    "missing_train = train_df[original_cols].isnull().sum().sum()\n",
    "missing_test = test_df[original_cols].isnull().sum().sum()\n",
    "\n",
    "print(f\"   Dataset de entrenamiento: {missing_train} valores nulos\")\n",
    "print(f\"   Dataset de prueba: {missing_test} valores nulos\")\n",
    "\n",
    "if missing_train == 0 and missing_test == 0:\n",
    "    print(f\"\\n   ‚úÖ Excelente: No hay valores faltantes en las variables originales.\")\n",
    "    print(f\"      No se requiere imputaci√≥n de datos.\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è Se detectaron valores faltantes. An√°lisis detallado:\")\n",
    "    if missing_train > 0:\n",
    "        print(f\"\\n   Columnas con valores nulos en TRAIN:\")\n",
    "        missing_cols_train = train_df[original_cols].isnull().sum()[train_df[original_cols].isnull().sum() > 0]\n",
    "        print(missing_cols_train)\n",
    "    if missing_test > 0:\n",
    "        print(f\"\\n   Columnas con valores nulos en TEST:\")\n",
    "        missing_cols_test = test_df[original_cols].isnull().sum()[test_df[original_cols].isnull().sum() > 0]\n",
    "        print(missing_cols_test)\n",
    "\n",
    "# Aclaraci√≥n sobre attack_category\n",
    "missing_attack_cat = test_df['attack_category'].isna().sum()\n",
    "if missing_attack_cat > 0:\n",
    "    print(f\"\\n   ‚ÑπÔ∏è Nota: {missing_attack_cat} observaciones en test no tienen 'attack_category' asignada.\")\n",
    "    print(f\"      Esto se debe a que contienen tipos de ataque nuevos no presentes en train.\")\n",
    "    print(f\"      Esta caracter√≠stica es intencional del dataset NSL-KDD para evaluar generalizaci√≥n.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc99cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de memoria y consideraciones computacionales\n",
    "print(\"\\nüíæ Uso de memoria:\")\n",
    "train_memory_mb = train_df.memory_usage(deep=True).sum() / 1024**2\n",
    "test_memory_mb = test_df.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "print(f\"   Dataset de entrenamiento: {train_memory_mb:.2f} MB\")\n",
    "print(f\"   Dataset de prueba: {test_memory_mb:.2f} MB\")\n",
    "print(f\"   Total en memoria: {train_memory_mb + test_memory_mb:.2f} MB\")\n",
    "\n",
    "if train_memory_mb + test_memory_mb < 100:\n",
    "    print(f\"\\n   ‚úÖ El dataset tiene un tama√±o manejable para an√°lisis completo.\")\n",
    "    print(f\"      No se requieren t√©cnicas especiales de optimizaci√≥n de memoria.\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ö†Ô∏è Dataset de tama√±o considerable. Se recomienda:\")\n",
    "    print(f\"      - Usar muestras estratificadas para visualizaciones pesadas\")\n",
    "    print(f\"      - Monitorear uso de RAM durante an√°lisis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ba7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vista previa de los datos\n",
    "print(\"\\nüìã Primeras 5 observaciones del dataset de entrenamiento:\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\nüìã √öltimas 5 observaciones del dataset de entrenamiento:\")\n",
    "display(train_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticos descriptivos de variables clave\n",
    "# Enfoc√°ndonos en las variables mencionadas en la Pregunta de Investigaci√≥n 1\n",
    "key_vars = ['duration', 'src_bytes', 'dst_bytes']\n",
    "\n",
    "print(\"\\nüìà Estad√≠sticos descriptivos de variables clave (Dataset completo):\")\n",
    "print(\"\\nEstas tres variables son centrales para la Pregunta de Investigaci√≥n 1.\")\n",
    "print(\"Se analizar√° si presentan diferencias significativas entre tr√°fico normal y ataques.\\n\")\n",
    "\n",
    "key_stats = train_df[key_vars].describe().T\n",
    "key_stats['range'] = key_stats['max'] - key_stats['min']\n",
    "key_stats['cv'] = (key_stats['std'] / key_stats['mean']) * 100  # Coeficiente de variaci√≥n\n",
    "\n",
    "display(key_stats[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'range', 'cv']])\n",
    "\n",
    "print(\"\\nüìä Interpretaci√≥n preliminar:\")\n",
    "for var in key_vars:\n",
    "    mean_val = train_df[var].mean()\n",
    "    median_val = train_df[var].median()\n",
    "    max_val = train_df[var].max()\n",
    "    cv = (train_df[var].std() / mean_val) * 100 if mean_val > 0 else 0\n",
    "    \n",
    "    print(f\"\\n   {var}:\")\n",
    "    print(f\"      - Rango: 0 a {max_val:,.0f}\")\n",
    "    print(f\"      - Media vs. Mediana: {mean_val:.2f} vs. {median_val:.2f}\")\n",
    "    \n",
    "    if median_val == 0:\n",
    "        print(f\"      - ‚ö†Ô∏è Mediana en 0: Indica que >50% de las conexiones tienen {var}=0\")\n",
    "    \n",
    "    if mean_val > median_val * 10 and median_val > 0:\n",
    "        print(f\"      - ‚ö†Ô∏è Asimetr√≠a positiva severa detectada (media >> mediana)\")\n",
    "        print(f\"      - Probable presencia de outliers extremos\")\n",
    "        print(f\"      - Recomendaci√≥n: Considerar transformaci√≥n logar√≠tmica para visualizaci√≥n\")\n",
    "    \n",
    "    if cv > 100:\n",
    "        print(f\"      - ‚ö†Ô∏è Coeficiente de variaci√≥n muy alto ({cv:.1f}%)\")\n",
    "        print(f\"      - Alta heterogeneidad en los datos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc3ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de Asimetr√≠a y Curtosis para variables clave\n",
    "print(\"\\nüìê Asimetr√≠a (Skewness) y Curtosis de variables clave:\")\n",
    "print(\"\\nEstas m√©tricas son fundamentales para:\")\n",
    "print(\"  ‚Ä¢ Determinar si las variables siguen distribuci√≥n normal (requisito de ANOVA)\")\n",
    "print(\"  ‚Ä¢ Justificar transformaciones antes de PCA (Pregunta 2)\")\n",
    "print(\"  ‚Ä¢ Decidir entre pruebas param√©tricas vs. no param√©tricas (Pregunta 1)\\n\")\n",
    "\n",
    "# Calcular para variables clave + algunas adicionales relevantes\n",
    "analysis_vars = ['duration', 'src_bytes', 'dst_bytes', 'count', 'srv_count', \n",
    "                 'serror_rate', 'dst_host_count', 'dst_host_srv_count']\n",
    "\n",
    "skewness_kurtosis = pd.DataFrame({\n",
    "    'Skewness': train_df[analysis_vars].skew(),\n",
    "    'Kurtosis': train_df[analysis_vars].kurtosis()\n",
    "})\n",
    "\n",
    "# Agregar interpretaci√≥n\n",
    "skewness_kurtosis['Interpretaci√≥n_Skew'] = skewness_kurtosis['Skewness'].apply(\n",
    "    lambda x: 'Sim√©trica' if abs(x) < 0.5 \n",
    "    else ('Asim√©trica moderada' if abs(x) < 1 \n",
    "          else ('Asim√©trica fuerte' if abs(x) < 2 \n",
    "                else 'Asim√©trica SEVERA'))\n",
    ")\n",
    "\n",
    "skewness_kurtosis['Interpretaci√≥n_Kurt'] = skewness_kurtosis['Kurtosis'].apply(\n",
    "    lambda x: 'Mesoc√∫rtica (normal)' if abs(x) < 1 \n",
    "    else ('Leptoc√∫rtica (colas pesadas)' if x > 1 \n",
    "          else 'Platic√∫rtica (colas ligeras)')\n",
    ")\n",
    "\n",
    "display(skewness_kurtosis.round(3))\n",
    "\n",
    "# Resumen interpretativo\n",
    "print(\"\\nüìä Resumen interpretativo:\")\n",
    "severe_skew = skewness_kurtosis[abs(skewness_kurtosis['Skewness']) > 2]\n",
    "if len(severe_skew) > 0:\n",
    "    print(f\"\\n   ‚ö†Ô∏è Variables con asimetr√≠a SEVERA (|skew| > 2): {len(severe_skew)}\")\n",
    "    print(f\"      {', '.join(severe_skew.index.tolist())}\")\n",
    "    print(f\"\\n   Implicaciones:\")\n",
    "    print(f\"      ‚Ä¢ NO cumplen supuesto de normalidad (ANOVA inv√°lido)\")\n",
    "    print(f\"      ‚Ä¢ REQUIEREN transformaci√≥n logar√≠tmica para visualizaci√≥n\")\n",
    "    print(f\"      ‚Ä¢ Se debe usar Kruskal-Wallis en lugar de ANOVA (Pregunta 1)\")\n",
    "    print(f\"      ‚Ä¢ Considerar transformaci√≥n antes de PCA para mejorar resultados\")\n",
    "\n",
    "heavy_tails = skewness_kurtosis[skewness_kurtosis['Kurtosis'] > 3]\n",
    "if len(heavy_tails) > 0:\n",
    "    print(f\"\\n   ‚ö†Ô∏è Variables con colas muy pesadas (kurtosis > 3): {len(heavy_tails)}\")\n",
    "    print(f\"      {', '.join(heavy_tails.index.tolist())}\")\n",
    "    print(f\"      Indica presencia de outliers extremos (no necesariamente err√≥neos)\")\n",
    "\n",
    "# Referencia te√≥rica\n",
    "print(\"\\nüìö Referencias te√≥ricas:\")\n",
    "print(\"   ‚Ä¢ Skewness = 0: Distribuci√≥n sim√©trica (ej: Normal)\")\n",
    "print(\"   ‚Ä¢ |Skewness| > 2: Asimetr√≠a severa ‚Üí distribuci√≥n NO normal\")\n",
    "print(\"   ‚Ä¢ Kurtosis = 0: Distribuci√≥n mesoc√∫rtica (ej: Normal)\")\n",
    "print(\"   ‚Ä¢ Kurtosis > 3: Colas muy pesadas (m√°s outliers que distribuci√≥n normal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ec6c2",
   "metadata": {},
   "source": [
    "### 2.1.1 Hallazgos de la Secci√≥n 2.1\n",
    "\n",
    "**Hallazgos estructurales:**\n",
    "\n",
    "1. **Calidad de datos:** No se detectaron valores nulos en las 43 variables originales del dataset (41 predictoras + attack_type + difficulty_level). La aparente ausencia de valores en `attack_category` del conjunto de prueba (13 casos) corresponde a tipos de ataque nuevos incluidos intencionalmente para evaluar generalizaci√≥n, no a valores faltantes reales.\n",
    "\n",
    "2. **Composici√≥n de variables:** \n",
    "   - **41 variables predictoras:** 38 num√©ricas (25 int64 + 13 float64) y 3 categ√≥ricas (protocol_type, service, flag)\n",
    "   - **Variables categ√≥ricas presentan diferentes cardinalidades:** protocol_type (3 categor√≠as), flag (11 categor√≠as), service (66 categor√≠as)\n",
    "   - Esta diversidad en cardinalidad requerir√° diferentes estrategias de codificaci√≥n en la preparaci√≥n de datos\n",
    "\n",
    "3. **Tama√±o computacionalmente manejable:** Con 24.33 MB de uso total de memoria (12.83 MB train + 11.50 MB test), el dataset completo puede procesarse en memoria sin necesidad de t√©cnicas de optimizaci√≥n especiales o procesamiento por lotes.\n",
    "\n",
    "**Observaciones sobre variables clave (duration, src_bytes, dst_bytes):**\n",
    "\n",
    "1. **Asimetr√≠a extrema y valores cero dominantes:**\n",
    "   - `duration` y `dst_bytes` tienen mediana = 0, indicando que m√°s del 50% de las conexiones presentan valor cero\n",
    "   - `src_bytes` tiene mediana de solo 44 bytes, mientras que su media es 24,330 bytes (553 veces mayor)\n",
    "   - Esta discrepancia media/mediana confirma distribuciones fuertemente sesgadas a la derecha\n",
    "\n",
    "2. **Heterogeneidad extrema:**\n",
    "   - Coeficientes de variaci√≥n extraordinariamente altos: duration (881%), src_bytes (9,909%), dst_bytes (2,544%)\n",
    "   - Valores superiores al 100% indican que la desviaci√≥n est√°ndar supera ampliamente la media\n",
    "   - Esta dispersi√≥n extrema es caracter√≠stica de datos de red que mezclan conexiones normales breves con ataques de gran volumen\n",
    "\n",
    "3. **Rangos de varios √≥rdenes de magnitud:**\n",
    "   - `src_bytes`: rango de 0 a 381 millones (9 √≥rdenes de magnitud)\n",
    "   - `dst_bytes`: rango de 0 a 5.1 millones\n",
    "   - `duration`: rango de 0 a 42,862 segundos (~12 horas)\n",
    "   - Estos rangos amplios sugieren presencia de outliers leg√≠timos (ataques masivos) que no deben eliminarse\n",
    "\n",
    "**M√©tricas de forma de distribuci√≥n (Skewness y Kurtosis):**\n",
    "\n",
    "1. **Confirmaci√≥n cuantitativa de no normalidad:**\n",
    "   - TODAS las variables clave presentan skewness > 2 (asimetr√≠a severa)\n",
    "   - Esto invalida formalmente el uso de pruebas param√©tricas como ANOVA\n",
    "   - Se confirma la necesidad de Kruskal-Wallis para la Pregunta 1\n",
    "\n",
    "2. **Colas extremadamente pesadas:**\n",
    "   - Valores de kurtosis muy superiores a 3 indican presencia abundante de valores extremos\n",
    "   - Estos outliers NO son errores de medici√≥n sino ataques leg√≠timos (ej: DoS masivos)\n",
    "   - Refuerza la decisi√≥n de mantener outliers en lugar de eliminarlos\n",
    "\n",
    "3. **Justificaci√≥n para transformaciones:**\n",
    "   - El skewness severo justifica transformaci√≥n log1p antes de PCA (Pregunta 2)\n",
    "   - Variables sin transformar har√≠an que PCA est√© dominado por outliers extremos\n",
    "   - La normalizaci√≥n/estandarizaci√≥n ser√° obligatoria antes de cualquier t√©cnica de ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc9c16",
   "metadata": {},
   "source": [
    "## 2.2 An√°lisis de la Variable Objetivo\n",
    "\n",
    "Esta secci√≥n examina la distribuci√≥n de la variable objetivo (tipos de ataque) en sus diferentes niveles de granularidad:\n",
    "- **Nivel 1 (Binario):** Normal vs. Ataque\n",
    "- **Nivel 2 (Categ√≥rico):** Normal, DoS, Probe, R2L, U2R\n",
    "- **Nivel 3 (Espec√≠fico):** 39 tipos de ataque individuales\n",
    "\n",
    "El an√°lisis del desbalance de clases es **cr√≠tico** para:\n",
    "- Interpretar correctamente los resultados de clasificaci√≥n (Pregunta 3)\n",
    "- Justificar el uso de m√©tricas especializadas (Recall, F1-Score) en lugar de Accuracy\n",
    "- Identificar categor√≠as problem√°ticas para el modelado (especialmente U2R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75222b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.1 Distribuci√≥n binaria: Normal vs. Ataque\n",
    "print(\"2.2.1 DISTRIBUCI√ìN BINARIA: NORMAL VS. ATAQUE\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Train\n",
    "train_binary = train_df['is_attack'].value_counts()\n",
    "train_binary_pct = train_df['is_attack'].value_counts(normalize=True) * 100\n",
    "\n",
    "axes[0].bar(['Normal', 'Ataque'], train_binary.values, \n",
    "            color=['#2ecc71', '#e74c3c'], edgecolor='black', alpha=0.8)\n",
    "axes[0].set_title('Distribuci√≥n Binaria - TRAIN', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Frecuencia', fontsize=11)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Agregar anotaciones de porcentaje\n",
    "for i, (val, pct) in enumerate(zip(train_binary.values, train_binary_pct.values)):\n",
    "    axes[0].text(i, val + 500, f'{val:,}\\n({pct:.1f}%)', \n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Test\n",
    "test_binary = test_df['is_attack'].value_counts()\n",
    "test_binary_pct = test_df['is_attack'].value_counts(normalize=True) * 100\n",
    "\n",
    "axes[1].bar(['Normal', 'Ataque'], test_binary.values, \n",
    "            color=['#2ecc71', '#e74c3c'], edgecolor='black', alpha=0.8)\n",
    "axes[1].set_title('Distribuci√≥n Binaria - TEST', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Frecuencia', fontsize=11)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Agregar anotaciones de porcentaje\n",
    "for i, (val, pct) in enumerate(zip(test_binary.values, test_binary_pct.values)):\n",
    "    axes[1].text(i, val + 500, f'{val:,}\\n({pct:.1f}%)', \n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabla resumen\n",
    "print(\"\\nüìä Resumen cuantitativo:\")\n",
    "binary_summary = pd.DataFrame({\n",
    "    'Train_Freq': train_binary.values,\n",
    "    'Train_%': train_binary_pct.values.round(2),\n",
    "    'Test_Freq': test_binary.values,\n",
    "    'Test_%': test_binary_pct.values.round(2)\n",
    "}, index=['Normal', 'Ataque'])\n",
    "\n",
    "print(binary_summary)\n",
    "\n",
    "print(\"\\nüí° Observaci√≥n:\")\n",
    "print(f\"   ‚Ä¢ En TRAIN: {train_binary_pct.values[0]:.1f}% Normal vs. {train_binary_pct.values[1]:.1f}% Ataque\")\n",
    "print(f\"   ‚Ä¢ En TEST: {test_binary_pct.values[0]:.1f}% Normal vs. {test_binary_pct.values[1]:.1f}% Ataque\")\n",
    "print(f\"   ‚Ä¢ El dataset est√° relativamente balanceado a nivel binario (‚àº50-50)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46742df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.2 Distribuci√≥n por categor√≠as de ataque\n",
    "print(\"2.2.2 DISTRIBUCI√ìN POR CATEGOR√çAS DE ATAQUE\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Train\n",
    "category_train = train_df['attack_category'].value_counts()\n",
    "category_train_pct = train_df['attack_category'].value_counts(normalize=True) * 100\n",
    "\n",
    "colors_train = [attack_colors[cat] for cat in category_train.index]\n",
    "\n",
    "axes[0].barh(category_train.index, category_train.values, \n",
    "             color=colors_train, edgecolor='black', alpha=0.85)\n",
    "axes[0].set_title('Distribuci√≥n por Categor√≠a - TRAIN', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Frecuencia', fontsize=11)\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Agregar anotaciones\n",
    "for i, (cat, val, pct) in enumerate(zip(category_train.index, category_train.values, category_train_pct.values)):\n",
    "    axes[0].text(val + 300, i, f'{val:,} ({pct:.2f}%)', \n",
    "                va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Test (filtrar NaN)\n",
    "category_test = test_df[test_df['attack_category'].notna()]['attack_category'].value_counts()\n",
    "category_test_pct = test_df[test_df['attack_category'].notna()]['attack_category'].value_counts(normalize=True) * 100\n",
    "\n",
    "colors_test = [attack_colors[cat] for cat in category_test.index]\n",
    "\n",
    "axes[1].barh(category_test.index, category_test.values, \n",
    "             color=colors_test, edgecolor='black', alpha=0.85)\n",
    "axes[1].set_title('Distribuci√≥n por Categor√≠a - TEST', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Frecuencia', fontsize=11)\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Agregar anotaciones\n",
    "for i, (cat, val, pct) in enumerate(zip(category_test.index, category_test.values, category_test_pct.values)):\n",
    "    axes[1].text(val + 300, i, f'{val:,} ({pct:.2f}%)', \n",
    "                va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabla comparativa\n",
    "print(\"\\nüìä Comparaci√≥n Train vs. Test:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Train_Freq': category_train,\n",
    "    'Train_%': category_train_pct.round(2),\n",
    "    'Test_Freq': category_test,\n",
    "    'Test_%': category_test_pct.round(2)\n",
    "})\n",
    "comparison_df = comparison_df.reindex(category_order)\n",
    "comparison_df['Diferencia_%'] = (comparison_df['Test_%'] - comparison_df['Train_%']).round(2)\n",
    "\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778418b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.3 An√°lisis detallado del desbalance de clases\n",
    "print(\"2.2.3 AN√ÅLISIS DE DESBALANCE DE CLASES\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è M√©tricas de desbalance en TRAIN:\")\n",
    "print(f\"\\n   Categor√≠a MAYORITARIA: {category_train.index[0]} con {category_train.values[0]:,} muestras ({category_train_pct.values[0]:.2f}%)\")\n",
    "print(f\"   Categor√≠a MINORITARIA: {category_train.index[-1]} con {category_train.values[-1]:,} muestras ({category_train_pct.values[-1]:.2f}%)\")\n",
    "\n",
    "# Calcular ratio de desbalance\n",
    "imbalance_ratio_train = category_train.values[0] / category_train.values[-1]\n",
    "print(f\"\\n   üìâ Ratio de desbalance (Mayoritaria/Minoritaria): {imbalance_ratio_train:.1f}:1\")\n",
    "print(f\"      ‚Üí La clase mayoritaria tiene {imbalance_ratio_train:.0f} veces m√°s ejemplos que U2R\")\n",
    "\n",
    "# An√°lisis por categor√≠a\n",
    "print(\"\\nüìä Caracterizaci√≥n del desbalance por categor√≠a (TRAIN):\")\n",
    "for cat in category_order:\n",
    "    freq = category_train.get(cat, 0)\n",
    "    pct = category_train_pct.get(cat, 0)\n",
    "    \n",
    "    if pct > 30:\n",
    "        status = \"‚úÖ BIEN REPRESENTADA\"\n",
    "    elif pct > 5:\n",
    "        status = \"üü° MODERADAMENTE REPRESENTADA\"\n",
    "    elif pct > 1:\n",
    "        status = \"‚ö†Ô∏è POCO REPRESENTADA\"\n",
    "    else:\n",
    "        status = \"üî¥ EXTREMADAMENTE RARA\"\n",
    "    \n",
    "    print(f\"   {cat:10s}: {freq:6,} muestras ({pct:5.2f}%) ‚Üí {status}\")\n",
    "\n",
    "# Comparaci√≥n Train vs Test\n",
    "print(\"\\nüîÑ Comparaci√≥n de distribuciones Train vs. Test:\")\n",
    "print(\"\\n   Categor√≠as con MAYOR cambio de proporci√≥n:\")\n",
    "for cat in category_order:\n",
    "    train_pct = category_train_pct.get(cat, 0)\n",
    "    test_pct = category_test_pct.get(cat, 0)\n",
    "    diff = test_pct - train_pct\n",
    "    \n",
    "    if abs(diff) > 5:  # Cambios mayores al 5%\n",
    "        direction = \"‚Üë AUMENT√ì\" if diff > 0 else \"‚Üì DISMINUY√ì\"\n",
    "        print(f\"   ‚Ä¢ {cat:10s}: {direction} {abs(diff):.2f} puntos porcentuales\")\n",
    "        print(f\"      Train: {train_pct:.2f}% ‚Üí Test: {test_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.4 An√°lisis de ataques espec√≠ficos dentro de cada categor√≠a\n",
    "print(\"2.2.4 ATAQUES ESPEC√çFICOS POR CATEGOR√çA\")\n",
    "\n",
    "# Crear visualizaci√≥n de los top ataques espec√≠ficos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Analizar cada categor√≠a de ataque (excepto Normal)\n",
    "attack_categories_analysis = ['DoS', 'Probe', 'R2L', 'U2R']\n",
    "\n",
    "for idx, category in enumerate(attack_categories_analysis):\n",
    "    # Filtrar ataques de esta categor√≠a en train\n",
    "    attacks_in_category = train_df[train_df['attack_category'] == category]['attack_type'].value_counts()\n",
    "    \n",
    "    # Crear gr√°fico de barras horizontal\n",
    "    colors_bar = [attack_colors[category]] * len(attacks_in_category)\n",
    "    \n",
    "    attacks_in_category.plot(kind='barh', ax=axes[idx], color=colors_bar, \n",
    "                             edgecolor='black', alpha=0.8)\n",
    "    axes[idx].set_title(f'Ataques Espec√≠ficos - {category}', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Frecuencia', fontsize=10)\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Agregar anotaciones de frecuencia\n",
    "    for i, (attack, freq) in enumerate(attacks_in_category.items()):\n",
    "        pct = (freq / attacks_in_category.sum()) * 100\n",
    "        axes[idx].text(freq + (attacks_in_category.max() * 0.02), i, \n",
    "                      f'{freq:,} ({pct:.1f}%)', \n",
    "                      va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabla resumen detallada\n",
    "print(\"\\nüìã Resumen detallado de ataques espec√≠ficos:\")\n",
    "for category in attack_categories_analysis:\n",
    "    print(f\"Categor√≠a: {category}\")\n",
    "    \n",
    "    # Train\n",
    "    train_attacks = train_df[train_df['attack_category'] == category]['attack_type'].value_counts()\n",
    "    train_attacks_pct = (train_attacks / train_attacks.sum() * 100).round(2)\n",
    "    \n",
    "    # Test\n",
    "    test_attacks = test_df[test_df['attack_category'] == category]['attack_type'].value_counts()\n",
    "    test_attacks_pct = (test_attacks / test_attacks.sum() * 100).round(2)\n",
    "    \n",
    "    # Combinar informaci√≥n\n",
    "    all_attacks = sorted(set(train_attacks.index) | set(test_attacks.index))\n",
    "    \n",
    "    print(f\"\\n{'Ataque':<20} {'Train':>15} {'Train %':>10} {'Test':>15} {'Test %':>10}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for attack in all_attacks:\n",
    "        train_freq = train_attacks.get(attack, 0)\n",
    "        train_pct = train_attacks_pct.get(attack, 0)\n",
    "        test_freq = test_attacks.get(attack, 0)\n",
    "        test_pct = test_attacks_pct.get(attack, 0)\n",
    "        \n",
    "        # Marcar ataques que solo aparecen en test\n",
    "        marker = \" üÜï\" if train_freq == 0 and test_freq > 0 else \"\"\n",
    "        \n",
    "        print(f\"{attack:<20} {train_freq:>15,} {train_pct:>9.2f}% {test_freq:>15,} {test_pct:>9.2f}%{marker}\")\n",
    "    \n",
    "    print(f\"\\nTotal en {category}: Train={train_attacks.sum():,}, Test={test_attacks.sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2.5 Visualizaci√≥n comparativa de distribuciones Train vs Test\n",
    "print(\"2.2.5 COMPARACI√ìN VISUAL TRAIN VS. TEST\")\n",
    "\n",
    "# Gr√°fico de barras agrupadas\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(category_order))\n",
    "width = 0.35\n",
    "\n",
    "# Preparar datos\n",
    "train_values = [category_train.get(cat, 0) for cat in category_order]\n",
    "test_values = [category_test.get(cat, 0) for cat in category_order]\n",
    "\n",
    "# Crear barras\n",
    "bars1 = ax.bar(x - width/2, train_values, width, label='Train', \n",
    "               color='#3498db', edgecolor='black', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, test_values, width, label='Test', \n",
    "               color='#e67e22', edgecolor='black', alpha=0.8)\n",
    "\n",
    "# Configuraci√≥n del gr√°fico\n",
    "ax.set_xlabel('Categor√≠a de Ataque', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Frecuencia', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comparaci√≥n de Distribuciones: Train vs. Test', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(category_order, fontsize=11)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Agregar valores encima de las barras\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height):,}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
